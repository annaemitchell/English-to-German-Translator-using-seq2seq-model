{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eng-german.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranabsarkar/Enlish-to-German-Translator-sing-seq2seq-model/blob/master/eng_german.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "HvGkgO__4ElQ",
        "colab_type": "code",
        "outputId": "2c9b2245-d445-4eb8-a823-de9dd972f22d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DkNppzfO5DEf",
        "colab_type": "code",
        "outputId": "5de79f66-4a23-41b4-f1c1-d94f0d1a1625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/german/english\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deu.txt\t\t\t english-german-test.pkl   model.h5\n",
            "english-german-both.pkl  english-german-train.pkl\n",
            "english-german.pkl\t german.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lgXeAR84_SQw",
        "colab_type": "code",
        "outputId": "beb9b347-ab4e-47d7-f6ed-b3d3874fcf66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2294
        }
      },
      "cell_type": "code",
      "source": [
        "# https://pypi.python.org/pypi/pydot\n",
        "!apt-get -qq install -y graphviz && pip install -q pydot\n",
        "import pydot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package fontconfig.\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 22298 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fontconfig_2.12.6-0ubuntu2_amd64.deb ...\n",
            "Unpacking fontconfig (2.12.6-0ubuntu2) ...\n",
            "Selecting previously unselected package libann0.\n",
            "Preparing to unpack .../01-libann0_1.1.2+doc-6_amd64.deb ...\n",
            "Unpacking libann0 (1.1.2+doc-6) ...\n",
            "Selecting previously unselected package libcdt5.\n",
            "Preparing to unpack .../02-libcdt5_2.40.1-2_amd64.deb ...\n",
            "Unpacking libcdt5 (2.40.1-2) ...\n",
            "Selecting previously unselected package libcgraph6.\n",
            "Preparing to unpack .../03-libcgraph6_2.40.1-2_amd64.deb ...\n",
            "Unpacking libcgraph6 (2.40.1-2) ...\n",
            "Selecting previously unselected package libjbig0:amd64.\n",
            "Preparing to unpack .../04-libjbig0_2.1-3.1build1_amd64.deb ...\n",
            "Unpacking libjbig0:amd64 (2.1-3.1build1) ...\n",
            "Selecting previously unselected package libtiff5:amd64.\n",
            "Preparing to unpack .../05-libtiff5_4.0.9-5_amd64.deb ...\n",
            "Unpacking libtiff5:amd64 (4.0.9-5) ...\n",
            "Selecting previously unselected package libwebp6:amd64.\n",
            "Preparing to unpack .../06-libwebp6_0.6.1-2_amd64.deb ...\n",
            "Unpacking libwebp6:amd64 (0.6.1-2) ...\n",
            "Selecting previously unselected package libxpm4:amd64.\n",
            "Preparing to unpack .../07-libxpm4_1%3a3.5.12-1_amd64.deb ...\n",
            "Unpacking libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Selecting previously unselected package libgd3:amd64.\n",
            "Preparing to unpack .../08-libgd3_2.2.5-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking libgd3:amd64 (2.2.5-4ubuntu0.2) ...\n",
            "Selecting previously unselected package libgts-0.7-5:amd64.\n",
            "Preparing to unpack .../09-libgts-0.7-5_0.7.6+darcs121130-4_amd64.deb ...\n",
            "Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
            "Selecting previously unselected package libpixman-1-0:amd64.\n",
            "Preparing to unpack .../10-libpixman-1-0_0.34.0-2_amd64.deb ...\n",
            "Unpacking libpixman-1-0:amd64 (0.34.0-2) ...\n",
            "Selecting previously unselected package libxcb-render0:amd64.\n",
            "Preparing to unpack .../11-libxcb-render0_1.13-1_amd64.deb ...\n",
            "Unpacking libxcb-render0:amd64 (1.13-1) ...\n",
            "Selecting previously unselected package libxcb-shm0:amd64.\n",
            "Preparing to unpack .../12-libxcb-shm0_1.13-1_amd64.deb ...\n",
            "Unpacking libxcb-shm0:amd64 (1.13-1) ...\n",
            "Selecting previously unselected package libcairo2:amd64.\n",
            "Preparing to unpack .../13-libcairo2_1.15.10-2_amd64.deb ...\n",
            "Unpacking libcairo2:amd64 (1.15.10-2) ...\n",
            "Selecting previously unselected package libltdl7:amd64.\n",
            "Preparing to unpack .../14-libltdl7_2.4.6-2_amd64.deb ...\n",
            "Unpacking libltdl7:amd64 (2.4.6-2) ...\n",
            "Selecting previously unselected package libthai-data.\n",
            "Preparing to unpack .../15-libthai-data_0.1.27-2_all.deb ...\n",
            "Unpacking libthai-data (0.1.27-2) ...\n",
            "Selecting previously unselected package libdatrie1:amd64.\n",
            "Preparing to unpack .../16-libdatrie1_0.2.10-7_amd64.deb ...\n",
            "Unpacking libdatrie1:amd64 (0.2.10-7) ...\n",
            "Selecting previously unselected package libthai0:amd64.\n",
            "Preparing to unpack .../17-libthai0_0.1.27-2_amd64.deb ...\n",
            "Unpacking libthai0:amd64 (0.1.27-2) ...\n",
            "Selecting previously unselected package libpango-1.0-0:amd64.\n",
            "Preparing to unpack .../18-libpango-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpango-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpangoft2-1.0-0:amd64.\n",
            "Preparing to unpack .../19-libpangoft2-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpangoft2-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpangocairo-1.0-0:amd64.\n",
            "Preparing to unpack .../20-libpangocairo-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpangocairo-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpathplan4.\n",
            "Preparing to unpack .../21-libpathplan4_2.40.1-2_amd64.deb ...\n",
            "Unpacking libpathplan4 (2.40.1-2) ...\n",
            "Selecting previously unselected package libgvc6.\n",
            "Preparing to unpack .../22-libgvc6_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgvc6 (2.40.1-2) ...\n",
            "Selecting previously unselected package libgvpr2.\n",
            "Preparing to unpack .../23-libgvpr2_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgvpr2 (2.40.1-2) ...\n",
            "Selecting previously unselected package liblab-gamut1.\n",
            "Preparing to unpack .../24-liblab-gamut1_2.40.1-2_amd64.deb ...\n",
            "Unpacking liblab-gamut1 (2.40.1-2) ...\n",
            "Selecting previously unselected package libxt6:amd64.\n",
            "Preparing to unpack .../25-libxt6_1%3a1.1.5-1_amd64.deb ...\n",
            "Unpacking libxt6:amd64 (1:1.1.5-1) ...\n",
            "Selecting previously unselected package libxmu6:amd64.\n",
            "Preparing to unpack .../26-libxmu6_2%3a1.1.2-2_amd64.deb ...\n",
            "Unpacking libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Selecting previously unselected package libxaw7:amd64.\n",
            "Preparing to unpack .../27-libxaw7_2%3a1.0.13-1_amd64.deb ...\n",
            "Unpacking libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Selecting previously unselected package graphviz.\n",
            "Preparing to unpack .../28-graphviz_2.40.1-2_amd64.deb ...\n",
            "Unpacking graphviz (2.40.1-2) ...\n",
            "Selecting previously unselected package libgts-bin.\n",
            "Preparing to unpack .../29-libgts-bin_0.7.6+darcs121130-4_amd64.deb ...\n",
            "Unpacking libgts-bin (0.7.6+darcs121130-4) ...\n",
            "Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
            "Setting up libpathplan4 (2.40.1-2) ...\n",
            "Setting up liblab-gamut1 (2.40.1-2) ...\n",
            "Setting up libxcb-render0:amd64 (1.13-1) ...\n",
            "Setting up libjbig0:amd64 (2.1-3.1build1) ...\n",
            "Setting up libdatrie1:amd64 (0.2.10-7) ...\n",
            "Setting up libtiff5:amd64 (4.0.9-5) ...\n",
            "Setting up libpixman-1-0:amd64 (0.34.0-2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libltdl7:amd64 (2.4.6-2) ...\n",
            "Setting up libann0 (1.1.2+doc-6) ...\n",
            "Setting up libxcb-shm0:amd64 (1.13-1) ...\n",
            "Setting up libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Setting up libxt6:amd64 (1:1.1.5-1) ...\n",
            "Setting up libgts-bin (0.7.6+darcs121130-4) ...\n",
            "Setting up libthai-data (0.1.27-2) ...\n",
            "Setting up libcdt5 (2.40.1-2) ...\n",
            "Setting up fontconfig (2.12.6-0ubuntu2) ...\n",
            "Regenerating fonts cache... done.\n",
            "Setting up libcgraph6 (2.40.1-2) ...\n",
            "Setting up libwebp6:amd64 (0.6.1-2) ...\n",
            "Setting up libcairo2:amd64 (1.15.10-2) ...\n",
            "Setting up libgvpr2 (2.40.1-2) ...\n",
            "Setting up libgd3:amd64 (2.2.5-4ubuntu0.2) ...\n",
            "Setting up libthai0:amd64 (0.1.27-2) ...\n",
            "Setting up libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Setting up libpango-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Setting up libpangoft2-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libpangocairo-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libgvc6 (2.40.1-2) ...\n",
            "Setting up graphviz (2.40.1-2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6jKfD0XlE84F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "metadata": {
        "id": "dnxc0i2601BW",
        "colab_type": "code",
        "outputId": "675f1981-800f-41b0-e6b9-3232b3ad1333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1872
        }
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/drive/My Drive/Colab Notebooks/german/english/deu.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, '/content/drive/My Drive/Colab Notebooks/german/english/english-german.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: /content/drive/My Drive/Colab Notebooks/german/english/english-german.pkl\n",
            "[hi] => [hallo]\n",
            "[hi] => [gru gott]\n",
            "[run] => [lauf]\n",
            "[wow] => [potzdonner]\n",
            "[wow] => [donnerwetter]\n",
            "[fire] => [feuer]\n",
            "[help] => [hilfe]\n",
            "[help] => [zu hulf]\n",
            "[stop] => [stopp]\n",
            "[wait] => [warte]\n",
            "[go on] => [mach weiter]\n",
            "[hello] => [hallo]\n",
            "[i ran] => [ich rannte]\n",
            "[i see] => [ich verstehe]\n",
            "[i see] => [aha]\n",
            "[i try] => [ich probiere es]\n",
            "[i won] => [ich hab gewonnen]\n",
            "[i won] => [ich habe gewonnen]\n",
            "[smile] => [lacheln]\n",
            "[cheers] => [zum wohl]\n",
            "[freeze] => [keine bewegung]\n",
            "[freeze] => [stehenbleiben]\n",
            "[got it] => [kapiert]\n",
            "[got it] => [verstanden]\n",
            "[got it] => [einverstanden]\n",
            "[he ran] => [er rannte]\n",
            "[he ran] => [er lief]\n",
            "[hop in] => [mach mit]\n",
            "[hug me] => [druck mich]\n",
            "[hug me] => [nimm mich in den arm]\n",
            "[hug me] => [umarme mich]\n",
            "[i fell] => [ich fiel]\n",
            "[i fell] => [ich fiel hin]\n",
            "[i fell] => [ich sturzte]\n",
            "[i fell] => [ich bin hingefallen]\n",
            "[i fell] => [ich bin gesturzt]\n",
            "[i know] => [ich wei]\n",
            "[i lied] => [ich habe gelogen]\n",
            "[i lost] => [ich habe verloren]\n",
            "[i paid] => [ich habe bezahlt]\n",
            "[i paid] => [ich zahlte]\n",
            "[i swim] => [ich schwimme]\n",
            "[im] => [ich bin jahre alt]\n",
            "[im] => [ich bin]\n",
            "[im ok] => [mir gehts gut]\n",
            "[im ok] => [es geht mir gut]\n",
            "[im up] => [ich bin wach]\n",
            "[im up] => [ich bin auf]\n",
            "[no way] => [unmoglich]\n",
            "[no way] => [das kommt nicht in frage]\n",
            "[no way] => [das gibts doch nicht]\n",
            "[no way] => [ausgeschlossen]\n",
            "[no way] => [in keinster weise]\n",
            "[really] => [wirklich]\n",
            "[really] => [echt]\n",
            "[really] => [im ernst]\n",
            "[thanks] => [danke]\n",
            "[try it] => [versuchs]\n",
            "[why me] => [warum ich]\n",
            "[ask tom] => [frag tom]\n",
            "[ask tom] => [fragen sie tom]\n",
            "[ask tom] => [fragt tom]\n",
            "[be cool] => [entspann dich]\n",
            "[be fair] => [sei nicht ungerecht]\n",
            "[be fair] => [sei fair]\n",
            "[be nice] => [sei nett]\n",
            "[be nice] => [seien sie nett]\n",
            "[beat it] => [geh weg]\n",
            "[beat it] => [hau ab]\n",
            "[beat it] => [verschwinde]\n",
            "[beat it] => [verdufte]\n",
            "[beat it] => [mach dich fort]\n",
            "[beat it] => [zieh leine]\n",
            "[beat it] => [mach dich vom acker]\n",
            "[beat it] => [verzieh dich]\n",
            "[beat it] => [verkrumele dich]\n",
            "[beat it] => [troll dich]\n",
            "[beat it] => [zisch ab]\n",
            "[beat it] => [pack dich]\n",
            "[beat it] => [mach ne fliege]\n",
            "[beat it] => [schwirr ab]\n",
            "[beat it] => [mach die sause]\n",
            "[beat it] => [scher dich weg]\n",
            "[beat it] => [scher dich fort]\n",
            "[call me] => [ruf mich an]\n",
            "[come in] => [komm herein]\n",
            "[come in] => [herein]\n",
            "[come on] => [komm]\n",
            "[come on] => [kommt]\n",
            "[come on] => [mach schon]\n",
            "[come on] => [macht schon]\n",
            "[come on] => [komm schon]\n",
            "[get tom] => [hol tom]\n",
            "[get out] => [raus]\n",
            "[get out] => [geh raus]\n",
            "[get out] => [geht raus]\n",
            "[go away] => [geh weg]\n",
            "[go away] => [hau ab]\n",
            "[go away] => [verschwinde]\n",
            "[go away] => [verdufte]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5zXt-AMoE2g2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "metadata": {
        "id": "Y43jfn_i01Bz",
        "colab_type": "code",
        "outputId": "2469a350-43ad-4813-b09d-2d619b0a6ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('/content/drive/My Drive/Colab Notebooks/german/english/english-german.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[9000:]\n",
        "# save\n",
        "save_clean_data(dataset, '/content/drive/My Drive/Colab Notebooks/german/english/english-german-both.pkl')\n",
        "save_clean_data(train, '/content/drive/My Drive/Colab Notebooks/german/english/english-german-train.pkl')\n",
        "save_clean_data(test, '/content/drive/My Drive/Colab Notebooks/german/english/english-german-test.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: /content/drive/My Drive/Colab Notebooks/german/english/english-german-both.pkl\n",
            "Saved: /content/drive/My Drive/Colab Notebooks/german/english/english-german-train.pkl\n",
            "Saved: /content/drive/My Drive/Colab Notebooks/german/english/english-german-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SzZ51cSkEsjg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Defining the model**"
      ]
    },
    {
      "metadata": {
        "id": "K4cocic501CE",
        "colab_type": "code",
        "outputId": "8af4c949-303d-4951-f842-71925bc43379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import pydot\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('/content/drive/My Drive/Colab Notebooks/german/english/english-german-both.pkl')\n",
        "train = load_clean_sentences('/content/drive/My Drive/Colab Notebooks/german/english/english-german-train.pkl')\n",
        "test = load_clean_sentences('/content/drive/My Drive/Colab Notebooks/german/english/english-german-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_output(trainY, ger_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_output(testY, ger_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(eng_vocab_size, ger_vocab_size, eng_length, ger_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 2309\n",
            "English Max Length: 5\n",
            "German Vocabulary Size: 3657\n",
            "German Max Length: 10\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 5, 256)            591104    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 10, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 10, 3657)          939849    \n",
            "=================================================================\n",
            "Total params: 2,581,577\n",
            "Trainable params: 2,581,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G8vE135JEk97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model Training**"
      ]
    },
    {
      "metadata": {
        "id": "gYLA9btr8vdB",
        "colab_type": "code",
        "outputId": "27550544-0e3d-4296-96dc-5e9d836d4206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2258
        }
      },
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "filename = '/content/drive/My Drive/Colab Notebooks/german/english/model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9000 samples, validate on 1000 samples\n",
            "Epoch 1/30\n",
            " - 12s - loss: 3.1259 - val_loss: 2.2375\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.23754, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 2/30\n",
            " - 10s - loss: 2.1379 - val_loss: 2.1138\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.23754 to 2.11375, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 3/30\n",
            " - 10s - loss: 2.0368 - val_loss: 2.0684\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.11375 to 2.06837, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 4/30\n",
            " - 10s - loss: 1.9613 - val_loss: 2.0085\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.06837 to 2.00846, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 5/30\n",
            " - 10s - loss: 1.8714 - val_loss: 1.9227\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.00846 to 1.92267, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 6/30\n",
            " - 10s - loss: 1.7911 - val_loss: 1.8772\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.92267 to 1.87720, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 7/30\n",
            " - 10s - loss: 1.7308 - val_loss: 1.8446\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.87720 to 1.84460, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 8/30\n",
            " - 10s - loss: 1.6811 - val_loss: 1.8134\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.84460 to 1.81337, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 9/30\n",
            " - 10s - loss: 1.6242 - val_loss: 1.7781\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.81337 to 1.77813, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 10/30\n",
            " - 10s - loss: 1.5572 - val_loss: 1.7179\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.77813 to 1.71792, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 11/30\n",
            " - 10s - loss: 1.4846 - val_loss: 1.6691\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.71792 to 1.66910, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 12/30\n",
            " - 10s - loss: 1.4110 - val_loss: 1.6229\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.66910 to 1.62286, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 13/30\n",
            " - 10s - loss: 1.3384 - val_loss: 1.5771\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.62286 to 1.57711, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 14/30\n",
            " - 10s - loss: 1.2688 - val_loss: 1.5356\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.57711 to 1.53556, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 15/30\n",
            " - 10s - loss: 1.2053 - val_loss: 1.4993\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.53556 to 1.49928, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 16/30\n",
            " - 10s - loss: 1.1451 - val_loss: 1.4717\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.49928 to 1.47174, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 17/30\n",
            " - 10s - loss: 1.0873 - val_loss: 1.4472\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.47174 to 1.44717, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 18/30\n",
            " - 10s - loss: 1.0313 - val_loss: 1.4239\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.44717 to 1.42386, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 19/30\n",
            " - 10s - loss: 0.9763 - val_loss: 1.3992\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.42386 to 1.39920, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 20/30\n",
            " - 10s - loss: 0.9241 - val_loss: 1.3830\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.39920 to 1.38297, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 21/30\n",
            " - 10s - loss: 0.8754 - val_loss: 1.3699\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.38297 to 1.36991, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 22/30\n",
            " - 10s - loss: 0.8277 - val_loss: 1.3545\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.36991 to 1.35454, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 23/30\n",
            " - 10s - loss: 0.7797 - val_loss: 1.3381\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.35454 to 1.33814, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 24/30\n",
            " - 10s - loss: 0.7366 - val_loss: 1.3191\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.33814 to 1.31905, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 25/30\n",
            " - 10s - loss: 0.6930 - val_loss: 1.3122\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.31905 to 1.31221, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 26/30\n",
            " - 10s - loss: 0.6530 - val_loss: 1.3012\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.31221 to 1.30123, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 27/30\n",
            " - 10s - loss: 0.6153 - val_loss: 1.2929\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.30123 to 1.29292, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 28/30\n",
            " - 10s - loss: 0.5828 - val_loss: 1.2829\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.29292 to 1.28289, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 29/30\n",
            " - 10s - loss: 0.5491 - val_loss: 1.2770\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.28289 to 1.27697, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n",
            "Epoch 30/30\n",
            " - 10s - loss: 0.5197 - val_loss: 1.2745\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.27697 to 1.27450, saving model to /content/drive/My Drive/Colab Notebooks/german/english/model.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa410a9d1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "gItjQnP71LfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycjRzCbgEdAR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model Evalution**"
      ]
    },
    {
      "metadata": {
        "id": "41fD_t5501CU",
        "colab_type": "code",
        "outputId": "a5c7f281-e5d2-400f-861d-a6aad27a4b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, ger_tokenizer, source)\n",
        "\t\traw_target, raw_src = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_target, raw_src, translation))\n",
        "      \n",
        "      \n",
        "\t\t\n",
        "\n",
        "  \n",
        " \n",
        "  \n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('/content/drive/My Drive/Colab Notebooks/german/english/english-german-both.pkl')\n",
        "train = load_clean_sentences('/content/drive/My Drive/Colab Notebooks/german/english/english-german-train.pkl')\n",
        "test = load_clean_sentences('/content/drive/My Drive/Colab Notebooks/german/english/english-german-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "testX = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "\n",
        "# load model\n",
        "model = load_model('/content/drive/My Drive/Colab Notebooks/german/english/model.h5')\n",
        "# test on some training sequences\n",
        "print('Evalution on the train data')\n",
        "evaluate_model(model, ger_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('Evalution on the test data')\n",
        "evaluate_model(model, ger_tokenizer, testX, test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evalution on the train data\n",
            "src=[tom swims], target=[tom schwimmt], predicted=[tom schwimmt]\n",
            "src=[mary looks hot], target=[maria sieht hei aus], predicted=[maria sieht aus aus]\n",
            "src=[are we safe], target=[sind wir sicher], predicted=[sind wir uns]\n",
            "src=[grab my hand], target=[nimm meine hand], predicted=[nimm meine]\n",
            "src=[i have won], target=[ich gewann], predicted=[ich habe sieger]\n",
            "src=[lets try it], target=[lasst es uns versuchen], predicted=[versuchen wir es]\n",
            "src=[good work tom], target=[gute arbeit tom], predicted=[gute braucht tom]\n",
            "src=[keep me posted], target=[halten sie mich auf dem laufenden], predicted=[halten sie mich zu]\n",
            "src=[tom is sitting], target=[tom sitzt], predicted=[tom sitzt]\n",
            "src=[count on it], target=[verlass dich drauf], predicted=[verlass dich drauf]\n",
            "Evalution on the test data\n",
            "src=[tom is vain], target=[tom ist eingebildet], predicted=[tom ist eitel]\n",
            "src=[you look sick], target=[du siehst krank aus], predicted=[du wirkst krank]\n",
            "src=[youre too big], target=[du bist zu gro], predicted=[du bist bist gro]\n",
            "src=[i need a hammer], target=[ich brauche einen hammer], predicted=[ich brauche eine zigarette]\n",
            "src=[youre so lazy], target=[ihr seid so faul], predicted=[du bist so faul faul]\n",
            "src=[are you busy], target=[habt ihr zu tun], predicted=[bist du beschaftigt]\n",
            "src=[im devastated], target=[ich bin am boden zerstort], predicted=[ich bin unschuldig]\n",
            "src=[i am injured], target=[ich bin verletzt], predicted=[ich bin kaputt]\n",
            "src=[i get up early], target=[ich stehe fruh auf], predicted=[ich gehe zu]\n",
            "src=[is he a teacher], target=[ist er lehrer], predicted=[ist er versalzen]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kum4EINzBj05",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}